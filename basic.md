# 数学入门

## 引子

### 方向导数

如果函数的增量与方向向量首末两点的距离比例存在，则称此为方向向量起点沿着向量方向的**方向导数**，记为：
$$\frac{\partial f}{\partial l}=\lim_{\rho \to 0} \frac{f(x+\Delta x,y+\Delta y)-f(x,y)}{\rho}$$

### 梯度

如果函数$z=f(x,y)$在平面域内具有连续的一阶偏导数，对于其中任意一点$P(x,y)$的梯度定义为：
$$grad f(x,y)=\frac{\partial f}{\partial x} \hat{i}+\frac{\partial f}{\partial y} \hat{j}$$

### 积分中值定理

如果函数$f(x)$在闭区间$[a,b]$上连续，则在$[a,b]$上至少存在一个点$\xi$，使：$$\int_{a}^{b}f(x)dx=f(\xi)(b-a)$$

### 牛顿-莱布尼茨公式

如果$F(x)$是连续函数$f(x)$在区间$[a,b]$上的一个原函数，则：$$\int_{a}^{b}f(x)dx=F(a)-F(b)$$

### 泰勒公式

若函数$f(x)$在包含$x_{0}$的某个开区间(a,b)上具有$n$阶的导数，那么对于任一$x \in (a,b)$，有：
$$
f(x)=\frac{f(x_{0})}{0!}+\frac{f'(x_{0})}{1!}(x-x_{0})^{2}+\frac{f''(x_{0})}{2!}(x-x_{0})+...+\frac{f^{(n)}(x_{0})}{n!}(x-x_{0})^{n}+R_{n}(x)
$$
其中：$R_{n}(x)=\frac{f^{n+1}(\varepsilon)}{(n+1)!}(x-x_{0})^{n+1}$称为泰勒余项，是泰勒多项式与原式的误差

### 拉格朗日乘子法

**——给定约束条件g(x,y)=c，求解f(x,y)在约束g下的极值**

设$(x^{*},y^{*})$是$f(x,y)$在曲线$g(x,y)=c$上的局部极小/极大点，且$\nabla g(x,y)\neq 0 $，则存在$\lambda^{*}$使得：
$$\begin{cases} 
g(x^{*},y^{*})=c \\
\nabla f(x^{*},y^{*})+\lambda^{*}\nabla g(x^{*},y^{*})=0 \\
\end{cases}$$
![](https://picgo2355sven.oss-cn-shenzhen.aliyuncs.com/math_pic/拉格朗日乘子法应用.png)
**当涉及多个自变量和多个约束条件时，拉格朗日乘子法同样适用**
![](https://picgo2355sven.oss-cn-shenzhen.aliyuncs.com/math_pic/多变量与多条件下的拉格朗日乘子法应用.jpg)

### 矩阵

- **行列式**
  
矩阵行列式代表按照该矩阵进行线性变换后，空间大小变换的倍数关系，例如原本面积为1的空间，经过$\begin{bmatrix}2&0\\0&2\end{bmatrix}$变换后，将变为面积为4的空间。
行列式的计算方式为主对角线方向数乘积之和减去副对角线方向数乘积之和
![](https://picgo2355sven.oss-cn-shenzhen.aliyuncs.com/math_pic/行列式计算公式.jpg)

- **转置**

$(AB)^{T}=B^{T}A^{T}$
$(A_{1}A_{2} \cdots A_{n})^{T}=A_{n}^{T} \cdots A_{2}^{T}A_{1}^{T}$

- **逆矩阵**

$A$为$n$阶方阵，若存在$n$阶方阵$B$，使得$AB=BA=I$，则$A,B$互逆，记作：$B=A^{-1}$
$$(\lambda A)^{-1}=\frac{1}{\lambda}A^{-1} $$

$$(AB)^{-1}=B^{-1}A^{-1}$$

- **矩阵的秩**
  - 代表线性变换后空间的维度
  - 一个矩阵A的列秩是A的线性独立的纵列的极大数
  - 行秩是A的线性无关的横行的极大数目


- **点积**
  - 求向量长度：$||x||=\sqrt{x \cdot x}$ 
  - 点积为零说明向量正交


### 矩阵特征值和特征向量
- 对于给定矩阵$A$，寻找一个常数$\lambda$和非零向量$x$，使得向量$x$被矩阵$A$作用后所得向量$Ax$于原向量x平行，且满足$Ax=\lambda x$,$x$称为特征向量，$\lambda$称为特征值
- 一个矩阵可能存在多个特征向量和特征值，一般$N×N$的矩阵中含有$n$个特征值和对应的特征向量
- 求解特征值和特征向量的方法：
  - 考虑到特征向量$x$满足$Ax=\lambda x$
  - 因此$Ax-\lambda x=0$
  - 提取$x$，可得$(A-\lambda E)x=0$，即特征向量经过$(A-\lambda E)$变换后为0向量
  - 因此$(A-\lambda E)$所代表的线性变换行列式为0
- 特征值分解：令$A$是一个$N×N$的方阵，且有$N$个线性无关的特征向量。这样，$A$可以被分解为$$A=U \Lambda U^{-1}$$ 其中，$U$为$N×N$方阵，且其第$i$列为$A$的特征向量。$\Lambda$是对角矩阵，其对角线上的元素为对应的特征值。
- 特征值分解后，可以通过提取特征值较大的特征向量，忽略特征值较小的特征向量，从而对矩阵进行信息提取。


### SVD矩阵分解（奇异值分解）
- **是特征分解在任意矩阵上的推广，解决了特征值分解中无法分解非方阵的问题。**
- 假设$M$是一个$m×n$阶矩阵，其中的元素全部属于域$K$，也就是实数域或复数域。如此则存在一个分解使得:
    $$
    M=U\Sigma V^{*}
    $$
    其中$U$是$m×m$阶酉矩阵；$\Sigma$是半正定$m×n$阶对角矩阵；而$V^{*}$，即$V$的共轭转置，是$n×n$阶酉矩阵。这样的分解就称作$M$的奇异值分解。$\Sigma$对角线上的元素$\Sigma_{i}$即为M的奇异值。
- 奇异值分解后，可以通过提取奇异值较大的奇异向量，忽略奇异值较小的奇异向量，从而对矩阵进行信息提取。

### 概率论
- 离散随机变量：通常使用概率表和柱状图表示概率
- 连续随机变量：通常使用概率密度和直方图表示概率，具体概率是概率密度函数在对应区间内的积分
  $$
  P(a<X<b)=\int_{a}^{b}f(x)dx
  $$
- 简单随机抽样：
  - 样本是相互独立的随机变量
  - 样本服从同分布
  - 联合概率密度和联合分布函数分别是单样本概率密度和单样本分布函数的乘积
- 似然函数
  - 为了在已知样本的情况下，求得最可能出现的参数值，引入**似然函数**和**极大似然估计**
  - 其中，似然函数是一种关于统计模型参数的函数。给定输出$x$时，关于参数$θ$的似然函数$L(θ|x)$（在数值上）等于给定参数$θ$后变量X的概率：$L(θ|x)=P(X=x|θ)$
    - 对于**离散型随机变量联合概率函数**所对应的似然函数为：
    $$L(\theta_{1},\theta_{2},\cdots ,\theta_{k})=\prod_{i=1}^{n}p(x_{i};\theta_{1},\theta_{2},\cdots ,\theta_{k})$$
    - 对于**连续型随机变量联合概率密度**所对应的似然函数为：
    $$L(\theta_{1},\theta_{2},\cdots ,\theta_{k})=\prod_{i=1}^{n}f(x_{i};\theta_{1},\theta_{2},\cdots ,\theta_{k})$$
    - 最大似然估计就是选择使$L(\theta_{1},\theta_{2},\cdots ,\theta_{k})$达到最大值的那一组$\theta$作为真实的估计。
    - 一般通过对$\prod_{i=1}^{n}f(x_{i};\theta_{1},\theta_{2},\cdots ,\theta_{k})$取对数的方式简化运算，将累乘运算转化为累加运算，并对想求解的$\theta$值求偏导数获取极值点，完成极大似然估计

- 二维随机变量
  - **联合分布**
    - 离散型：若$(X,Y)$为随机变量，对于任意实数$x,y$，$F(x,y)=P{(X \le x)\cap(Y \le y)}$表示随机点$(X,Y)$在以$(x,y)$为顶点且位于该点左下方无穷矩形的概率，且有
    $$
    P(x_{1}<X\le x_{2},y_{1}<Y\le y_{2})=F( x_{2},y_{2})-F(x_{1},y_{2})-F(x_{2},y_{1})+F(x_{1},y_{1})
    $$
    - 连续型：对于二维随机变量$(X,Y)$的分布函数$F(x,y)$，如果存在非负函数$f(x,y)$对于任意$x,y$有：$F(x,y)=\int_{-\infty}^{y}\int_{-\infty}^{x}f(u,v)dudv$，称$(X,Y)$为连续型的二维随机变量，$f(x,y)$为其概率密度，且有：
    $$
    P{(X,Y)\in G}=\int_{G}\int_{G}f(x,y)dxdy
    $$
  - **边缘分布**
    - 指在概率论和统计学的多维随机变量中，只包含其中部分变量的概率分布

- 数学期望：反映随机变量的取值水平
  - 离散型随机变量$Z=g(x,y)$的期望为：
$$
E(Z)=E(g(x,y))=\sum_{j=1}^{\infin} \sum_{i=1}^{\infin} g(x_{i},y_{j}))p_{ij}
$$
  - 连续型随机变量$Z=g(x,y)$概率密度为$z=f(x,y)$，设$\int_{-\infin}^{+\infin} \int_{-\infin}^{+\infin} g(x,y)f(x,y)dxdy$绝对收敛，有
$$
E(Z)=E(g(x,y))=\int_{-\infin}^{+\infin} \int_{-\infin}^{+\infin} g(x,y)f(x,y)dxdy
$$
- 方差：衡量随机变量相对于数学期望的分散程度$D(X)=E(X-E(X))^{2}=E(X^{2})-[E(X)^{2}]$
- 大数定理：重复实验多次后随机事件的频率近似于其概率
- 马尔科夫不等式：
$$
P(X \ge a)\le\frac{(E(X))}{a} 
$$
![](https://picgo2355sven.oss-cn-shenzhen.aliyuncs.com/math_pic/马尔科夫不等式证明.jpg)
- 切比雪夫不等式：
$$
P(|X-E(X)|\ge \epsilon)\le \frac{\sigma^{2}}{\epsilon^{2}}
$$
![](https://picgo2355sven.oss-cn-shenzhen.aliyuncs.com/math_pic/切比雪夫不等式证明.jpg)

### 聚类分析
- 层次聚类：试图在不同层次对数据集进行划分，从而形成树形的聚类结构。
- k-means聚类算法
  - 预将数据分为K组，则随机选取K个对象作为初始的聚类中心，然后计算每个对象与各个种子聚类中心之间的距离，把每个对象分配给距离它最近的聚类中心。
  - 聚类中心以及分配给它们的对象就代表一个聚类。
  - 每分配一个样本，聚类的聚类中心会根据聚类中现有的对象被重新计算。
  - 这个过程将不断重复直到满足某个终止条件。
  - 终止条件可以是没有（或最小数目）对象被重新分配给不同的聚类，没有（或最小数目）聚类中心再发生变化，误差平方和局部最小。

### DBSCAN聚类